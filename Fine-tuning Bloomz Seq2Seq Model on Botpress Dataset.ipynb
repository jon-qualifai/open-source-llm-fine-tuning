{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a14df0b-e6d6-4919-ab38-3af078cf9785",
   "metadata": {},
   "source": [
    "# Fine-tuning Bloomz Seq2Seq Model on Botpress Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd740094-d155-41b4-925e-eb7d648043bb",
   "metadata": {},
   "source": [
    "## Development Environment and Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da611756-ec45-477e-9f0d-45e5682536e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1daffe6-bef5-4465-b409-6e79ca277549",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.48.0 in /opt/conda/lib/python3.7/site-packages (2.127.0)\n",
      "Requirement already satisfied: transformers==4.12.3 in /opt/conda/lib/python3.7/site-packages (4.12.3)\n",
      "Requirement already satisfied: datasets[s3]==1.18.3 in /opt/conda/lib/python3.7/site-packages (1.18.3)\n",
      "Requirement already satisfied: sacrebleu in /opt/conda/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.97)\n",
      "Collecting transformers[sentencepiece]\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (4.13.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (1.21.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (4.64.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (0.0.53)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (0.11.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (2022.10.31)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.3) (22.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (3.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (3.8.3)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (10.0.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (2022.11.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (1.3.5)\n",
      "Requirement already satisfied: botocore in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (1.29.42)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (0.4.2)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from datasets[s3]==1.18.3) (1.26.42)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.2.0)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.1.5)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.3.0)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (22.1.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.7.5)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (3.20.2)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (4.9.1)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.4.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (2.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.3.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.34.2)\n",
      "  Using cached transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "  Using cached transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
      "  Using cached transformers-4.23.0-py3-none-any.whl (5.3 MB)\n",
      "  Using cached transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
      "  Using cached transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
      "  Using cached transformers-4.22.0-py3-none-any.whl (4.9 MB)\n",
      "  Using cached transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
      "  Using cached transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
      "  Using cached transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
      "  Using cached transformers-4.21.0-py3-none-any.whl (4.7 MB)\n",
      "  Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "  Using cached transformers-4.20.0-py3-none-any.whl (4.4 MB)\n",
      "  Using cached transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
      "  Using cached transformers-4.19.3-py3-none-any.whl (4.2 MB)\n",
      "  Using cached transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "  Using cached transformers-4.19.1-py3-none-any.whl (4.2 MB)\n",
      "  Using cached transformers-4.19.0-py3-none-any.whl (4.2 MB)\n",
      "  Using cached transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "  Using cached transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
      "  Using cached transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
      "  Using cached transformers-4.16.1-py3-none-any.whl (3.5 MB)\n",
      "  Using cached transformers-4.16.0-py3-none-any.whl (3.5 MB)\n",
      "  Using cached transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
      "  Using cached transformers-4.14.1-py3-none-any.whl (3.4 MB)\n",
      "  Using cached transformers-4.13.0-py3-none-any.whl (3.3 MB)\n",
      "  Using cached transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
      "  Using cached transformers-4.12.4-py3-none-any.whl (3.1 MB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->datasets[s3]==1.18.3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3->datasets[s3]==1.18.3) (0.6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore->datasets[s3]==1.18.3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore->datasets[s3]==1.18.3) (1.26.13)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==1.18.3) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==1.18.3) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==1.18.3) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==1.18.3) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==1.18.3) (6.0.3)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==1.18.3) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets[s3]==1.18.3) (2.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.12.3) (3.11.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker>=2.48.0) (1.14.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.3) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.3) (2022.9.24)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets[s3]==1.18.3) (2019.3)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (0.3.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (1.7.6.6)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.3) (7.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.3) (0.14.1)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from schema->sagemaker>=2.48.0) (0.6.0.post1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.12.3\" \"datasets[s3]==1.18.3\" sacrebleu torch sentencepiece transformers[sentencepiece] --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b731354f-8956-4fae-875c-49d8263c70cd",
   "metadata": {},
   "source": [
    "### Development environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d0158f-d10d-4225-8043-4fc35af70ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12.3\n"
     ]
    }
   ],
   "source": [
    "import sagemaker.huggingface\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "print(transformers.__version__)\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"]=\"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f99f72-d1b2-46c5-ad59-e32dbb23d689",
   "metadata": {},
   "source": [
    "### Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb1e8849-b26b-4280-b9a2-6e84f3da1028",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::352302020638:role/service-role/SageMaker-MLEngineer\n",
      "sagemaker bucket: sagemaker-eu-west-2-352302020638\n",
      "sagemaker session region: eu-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfcabcd-8ed4-4e73-adb8-822c1997040a",
   "metadata": {},
   "source": [
    "## Loading the fine-tuning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95ecf832-e602-45c7-8b14-6f5bd81b96e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('ob-loose-jun28-sm.jsonl', 'r') as json_file:\n",
    "    jsonl = json_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4138bf10-f9c3-466f-b515-1fe58acf5ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Format the data in the required format\n",
    "data = [{'sequences':json.loads(t)} for t in jsonl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02ac0df3-f7bd-4c9f-a1d5-b7dfb511f83c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the files into train, validation and test\n",
    "train, test = train_test_split(data, test_size=0.1)\n",
    "\n",
    "validation, test = train_test_split(test, test_size=0.5)\n",
    "\n",
    "train_df = pd.DataFrame(train)\n",
    "validation_df = pd.DataFrame(validation)\n",
    "test_df = pd.DataFrame(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d80cfd83-d334-4d1c-ab38-23b8718e91e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write the train, validation & test sets\n",
    "train_df.to_json(path_or_buf='ob-loose-jun28-sm_train.jsonl', orient='records', lines=True)\n",
    "validation_df.to_json(path_or_buf='ob-loose-jun28-sm_validation.jsonl', orient='records', lines=True)\n",
    "test_df.to_json(path_or_buf='ob-loose-jun28-sm_test.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f21cc6a-b88c-4097-adb1-9bcb6f889dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f53d8eda4a457a87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-f53d8eda4a457a87/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5fb98e70984fd4ba0474c8ab732082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a7cec77f4c403dbdf91c9342615088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-f53d8eda4a457a87/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b7b083111b4fbab2048d4aaf7c4478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the datasets as HuggingFace datasets\n",
    "base_path = 'ob-loose-jun28-sm_'\n",
    "raw_datasets = load_dataset(\"json\", data_files={\"train\": base_path + \"train.jsonl\", \"validation\": base_path + \"validation.jsonl\", \"test\": base_path + \"test.jsonl\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19436778-68ee-4a16-90c6-bc13b391ada5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequences'],\n",
       "        num_rows: 907\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sequences'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sequences'],\n",
       "        num_rows: 51\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf38509-a82e-4cc8-acfa-6510257bf163",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48bcc240-95ed-4520-96a2-d1f8e6ddc7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model checkpoint, start with mt0-small\n",
    "model_checkpoint = \"bigscience/mt0-small\"\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/datasets/fine_tuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c58bebe4-852f-4cb8-9027-533fc403fa6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "526573db-ad58-45d2-941e-50f5660eeb18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"\"\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source = \"prompt\"\n",
    "target = \"completion\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source] for ex in examples[\"sequences\"]]\n",
    "    targets = [ex[target] for ex in examples[\"sequences\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75501d4c-46d2-4b92-a6f6-6e16f5f025a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x7fb4eaa54b90> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97faefcb3794df582b26e8f02918f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8c1761cf9342b9922c1a55534a8d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d5c5e50ecb429caa13b7b4aaf385d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6727e52d-73ba-411a-9033-b4fd8365586b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequences', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 907\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sequences', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sequences', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 51\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0f59458-9fcd-4a00-8af0-e47eb00f6cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set format for pytorch\n",
    "train_dataset =  tokenized_datasets['train']\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = tokenized_datasets['test']\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c81dda1-845e-4e47-8cb4-97885b26c22d",
   "metadata": {},
   "source": [
    "### Uploading data to sagemaker_session_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56aa9bac-89ad-473c-b25c-e09233a45827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893a625-aa45-4c8f-a406-d90f5e97a7ff",
   "metadata": {},
   "source": [
    "# Finetuning the BLOOMZ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53150c67-ea36-4611-abf6-7b3f0d541888",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Trainer, AutoTokenizer, DataCollatorForSeq2Seq\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score, precision_recall_fscore_support\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_from_disk, load_metric\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m5e-5\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--n_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--training_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    args, _ = parser.parse_known_args()\n",
      "\n",
      "    \u001b[37m# Set up logging\u001b[39;49;00m\n",
      "    logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "\n",
      "    logging.basicConfig(\n",
      "        level=logging.getLevelName(\u001b[33m\"\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        handlers=[logging.StreamHandler(sys.stdout)],\n",
      "        \u001b[36mformat\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# load datasets\u001b[39;49;00m\n",
      "    train_dataset = load_from_disk(args.training_dir)\n",
      "    test_dataset = load_from_disk(args.test_dir)\n",
      "\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded train_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(train_dataset)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded test_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(test_dataset)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# compute metrics function for sequence to sequence generation\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# evaluation metric - BLEU\u001b[39;49;00m\n",
      "    metric = load_metric(\u001b[33m\"\u001b[39;49;00m\u001b[33msacrebleu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(eval_preds):\n",
      "        preds, labels = eval_preds\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(preds, \u001b[36mtuple\u001b[39;49;00m):\n",
      "            preds = preds[\u001b[34m0\u001b[39;49;00m]\n",
      "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        \u001b[37m# Replace -100 in the labels as we can't decode them.\u001b[39;49;00m\n",
      "        labels = np.where(labels != -\u001b[34m100\u001b[39;49;00m, labels, tokenizer.pad_token_id)\n",
      "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        \u001b[37m# Some simple post-processing\u001b[39;49;00m\n",
      "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
      "        result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
      "        result = {\u001b[33m\"\u001b[39;49;00m\u001b[33mbleu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: result[\u001b[33m\"\u001b[39;49;00m\u001b[33mscore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]}\n",
      "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) \u001b[34mfor\u001b[39;49;00m pred \u001b[35min\u001b[39;49;00m preds]\n",
      "        result[\u001b[33m\"\u001b[39;49;00m\u001b[33mgen_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = np.mean(prediction_lens)\n",
      "        result = {k: \u001b[36mround\u001b[39;49;00m(v, \u001b[34m4\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m k, v \u001b[35min\u001b[39;49;00m result.items()}\n",
      "        \u001b[34mreturn\u001b[39;49;00m result\n",
      "\n",
      "    \u001b[37m# download model from model hub\u001b[39;49;00m\n",
      "    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name)\n",
      "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
      "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
      "\n",
      "    \u001b[37m# define training args\u001b[39;49;00m\n",
      "    training_args = TrainingArguments(\n",
      "        output_dir=args.model_dir,\n",
      "        num_train_epochs=args.epochs,\n",
      "        per_device_train_batch_size=args.train_batch_size,\n",
      "        per_device_eval_batch_size=args.eval_batch_size,\n",
      "        warmup_steps=args.warmup_steps,\n",
      "        evaluation_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        logging_dir=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.output_data_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/logs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        learning_rate=\u001b[36mfloat\u001b[39;49;00m(args.learning_rate),\n",
      "        weight_decay=\u001b[34m0.01\u001b[39;49;00m,\n",
      "        save_total_limit=\u001b[34m3\u001b[39;49;00m,\n",
      "        predict_with_generate=\u001b[34mTrue\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    \u001b[37m# create Trainer instance\u001b[39;49;00m\n",
      "    trainer = Trainer(\n",
      "        model=model,\n",
      "        args=training_args,\n",
      "        compute_metrics=compute_metrics,\n",
      "        train_dataset=train_dataset,\n",
      "        eval_dataset=test_dataset,\n",
      "        tokenizer=tokenizer,\n",
      "        data_collator=data_collator,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# train model\u001b[39;49;00m\n",
      "    trainer.train()\n",
      "\n",
      "    \u001b[37m# evaluate model\u001b[39;49;00m\n",
      "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
      "\n",
      "    \u001b[37m# writes eval result to file which can be accessed later in s3 ouput\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.output_data_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33meval_results.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m writer:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Eval results *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m \u001b[36msorted\u001b[39;49;00m(eval_result.items()):\n",
      "            writer.write(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mkey\u001b[33m}\u001b[39;49;00m\u001b[33m = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvalue\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Saves the model to s3\u001b[39;49;00m\n",
      "    trainer.save_model(args.model_dir)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./scripts/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8b992-a530-4e7f-8189-efde39577fe4",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2144024-d02f-4631-85da-4533d4c7565b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'train_batch_size': 4,\n",
    "                 'model_name': model_checkpoint\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e353bb71-e25e-4811-a200-16613edaa9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.12',\n",
    "                            pytorch_version='1.9',\n",
    "                            py_version='py38',\n",
    "                            hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a681977-30d9-45ca-9d0b-2fe7bfc00589",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-01-04-17-37-43-190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-04 17:37:43 Starting - Starting the training job...\n",
      "2023-01-04 17:38:09 Starting - Preparing the instances for training.........\n",
      "2023-01-04 17:39:26 Downloading - Downloading input data...\n",
      "2023-01-04 17:39:55 Training - Downloading the training image.............."
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779489be-5cfd-41b8-b9fe-1c76a3e4c596",
   "metadata": {},
   "source": [
    "## Deploying the endpoint for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c4caa5-b86f-4656-a357-19df13dc012f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c230f-8530-4fa4-9263-56e5e3e903c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_input= {\"prompt\":\"Rephrase and join the sentences to remove repetition and sound more human without changing the wording and semantics.\\nThe only exception is that you are allowed to rephrase from the user query the <|NOT_SURE|> parts.\\n\\n###\\n\\nQuestion: Did you know that || You mentioned that patients can schedule follow-up appointments at the doctor\\u2019s office, what about first-time patients that want to schedule appointments in person? || We offer that as well!\\nRobotic Answer: <|NOT_SURE|> <|Patients can schedule an appointment at the lobby at every Hospital X facility|>\\nHuman Answer:\"}\n",
    "\n",
    "predictor.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7893dd4-649f-4919-b231-278e1e45de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba811b3-2ecb-41ec-816e-8ebe51c36807",
   "metadata": {},
   "source": [
    "# Extra information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b6911-9e1d-4132-88d9-5706ae0aa005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d83ed79-33c5-42a3-b985-64d47737f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
